---
layout: about
title: About
permalink: /
subtitle: <b>Workshop at the Conference on Neural Information Processing Systems (NeurIPS) 2024</b>

profile:
  align: right

news: false # includes a list of news items. TODO: set to true when needed
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

The first NeurIPS workshop on **Time Series in the Age of Large Models** will be held at the Vancouver Convention Center on **December 14/15, 2024**.
We look forward to welcoming you in Vancouver. 

### Introduction

Foundation models have significantly changed the approach to building machine learning models in areas such as natural language processing, where models are pretrained on large amounts of diverse data and then adapted for downstreams tasks, often in a _zero-shot_ fashion.
Such pretrained models have started to gain traction in the time series community and several recent works have developed and open-sourced foundation models for time series tasks, particularly forecasting.
These works have opened up new research directions and challenges.
This workshop seeks to provide a forum for researchers and practitioners to understand the progress so far and push the frontier of research on pretrained time series models. 

The key topics of this workshop include, but are not limited to:
- Building Foundation Models for Time Series Data
- Analyzing Existing Pretrained Time Series Models
- Critiques of Foundation Models for Time Series
- Faster and Better Inference Schemes for Autoregressive FTSMs
- Leveraging Pretrained Models of Other Modalities for Time Series
- Multimodal Time Series Models
- Large-Scale Time Series Datasets and Benchmarks
- Time Series Evaluation
- Real-World Applications of FTSMs

Please see the [Call for Papers](/call-for-papers/) for details.

### Key Information

- Submission link: TBD
- Submission deadline: Sep 15, 2024 (11:59 PM AoE)
- Acceptance notification: Oct 14, 2024
- Camera ready deadline: Nov 25, 2024


